{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":97919,"databundleVersionId":11694977,"sourceType":"competition"}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Grammar Scoring Engine for Spoken Data\n\n#  Brief Report","metadata":{}},{"cell_type":"markdown","source":"## Objective\nPredict a continuous grammar score (0â€“5) for each spoken audio sample using Wav2Vec2 embeddings and Ridge Regression.\n\n## Approach\nWe used Wav2Vec2, a powerful pretrained model from Facebook, to extract deep audio embeddings from .wav files. These embeddings were fed into a Ridge Regression model for predicting grammar scores.","metadata":{}},{"cell_type":"markdown","source":"###  **Pipeline Overview**\n\n1. **Preprocessing**:\n   - Converted `.wav` to 16kHz mono audio\n   - Used `facebook/wav2vec2-base-960h` for extracting 768-dim features\n\n2. **Model**:\n   - Ridge Regression (suitable for small datasets and continuous outputs)\n\n3. **Training**:\n   - Split data into training and validation sets (80-20 split)\n   - Trained on 444 samples using CPU (no GPU used)","metadata":{}},{"cell_type":"markdown","source":"### **Evaluation Results**\n\n- **Validation RMSE**: 0.9027\n- **Pearson Correlation**: 0.6442","metadata":{}},{"cell_type":"markdown","source":"###  **Submission Format**\n\n| filename       | label |\n|----------------|-------|\n| audio_001.wav  | 3.42  |\n| audio_002.wav  | 2.78  |\n---","metadata":{}},{"cell_type":"code","source":"!pip install transformers librosa torch --quiet\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport librosa\nimport torch\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_squared_error\nfrom scipy.stats import pearsonr\nfrom transformers import Wav2Vec2Processor, Wav2Vec2Model\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T14:39:25.177484Z","iopub.execute_input":"2025-04-06T14:39:25.177884Z","iopub.status.idle":"2025-04-06T14:39:58.779039Z","shell.execute_reply.started":"2025-04-06T14:39:25.177848Z","shell.execute_reply":"2025-04-06T14:39:58.778377Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"base_path = '/kaggle/input/shl-intern-hiring-assessment/dataset/'\n\ntrain_df = pd.read_csv(base_path + 'train.csv')\ntest_df = pd.read_csv(base_path + 'test.csv')\nsample_submission = pd.read_csv(base_path + 'sample_submission.csv')\n\ntrain_audio_path = base_path + 'audios_train/'\ntest_audio_path = base_path + 'audios_test/'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T14:40:45.491303Z","iopub.execute_input":"2025-04-06T14:40:45.491930Z","iopub.status.idle":"2025-04-06T14:40:45.516375Z","shell.execute_reply.started":"2025-04-06T14:40:45.491903Z","shell.execute_reply":"2025-04-06T14:40:45.515548Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"from transformers.utils import logging\nlogging.set_verbosity_error()\nprocessor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\nmodel = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\")\nmodel.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T14:40:50.478454Z","iopub.execute_input":"2025-04-06T14:40:50.478764Z","iopub.status.idle":"2025-04-06T14:40:57.094940Z","shell.execute_reply.started":"2025-04-06T14:40:50.478738Z","shell.execute_reply":"2025-04-06T14:40:57.094194Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/159 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cfb500d9e96f4344b3efbb38e5e6e550"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/163 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da4f83085cd24e7a91729467ed6ddaa7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.60k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb49ec58e9fc417d908a7b0a7b826a6f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/291 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a0366f7e34848749d1444ed8705f028"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb8936b76fb64456be46f32b90f6759e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/378M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ddee0064ea9b4730b0eed90cf2bf1ab3"}},"metadata":{}},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"Wav2Vec2Model(\n  (feature_extractor): Wav2Vec2FeatureEncoder(\n    (conv_layers): ModuleList(\n      (0): Wav2Vec2GroupNormConvLayer(\n        (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n        (activation): GELUActivation()\n        (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)\n      )\n      (1-4): 4 x Wav2Vec2NoLayerNormConvLayer(\n        (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n        (activation): GELUActivation()\n      )\n      (5-6): 2 x Wav2Vec2NoLayerNormConvLayer(\n        (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n        (activation): GELUActivation()\n      )\n    )\n  )\n  (feature_projection): Wav2Vec2FeatureProjection(\n    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n    (projection): Linear(in_features=512, out_features=768, bias=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (encoder): Wav2Vec2Encoder(\n    (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n      (conv): ParametrizedConv1d(\n        768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n        (parametrizations): ModuleDict(\n          (weight): ParametrizationList(\n            (0): _WeightNorm()\n          )\n        )\n      )\n      (padding): Wav2Vec2SamePadLayer()\n      (activation): GELUActivation()\n    )\n    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n    (layers): ModuleList(\n      (0-11): 12 x Wav2Vec2EncoderLayer(\n        (attention): Wav2Vec2SdpaAttention(\n          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n        )\n        (dropout): Dropout(p=0.1, inplace=False)\n        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (feed_forward): Wav2Vec2FeedForward(\n          (intermediate_dropout): Dropout(p=0.1, inplace=False)\n          (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n          (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n          (output_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      )\n    )\n  )\n)"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"def extract_wav2vec2_embedding(file_path):\n    y, sr = librosa.load(file_path, sr=16000)\n    inputs = processor(y, return_tensors=\"pt\", sampling_rate=16000).to(device)\n    \n    with torch.no_grad():\n        outputs = model(**inputs)\n    \n    return outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T14:40:59.729583Z","iopub.execute_input":"2025-04-06T14:40:59.729975Z","iopub.status.idle":"2025-04-06T14:40:59.734834Z","shell.execute_reply.started":"2025-04-06T14:40:59.729935Z","shell.execute_reply":"2025-04-06T14:40:59.733840Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\nX_train_feats = []\ny_train_labels = []\n\nfor idx, row in train_df.iterrows():\n    file_path = os.path.join(train_audio_path, row['filename'])\n    features = extract_wav2vec2_embedding(file_path)\n    X_train_feats.append(features)\n    y_train_labels.append(row['label'])\n\nX = np.array(X_train_feats)\ny = np.array(y_train_labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T14:45:36.615787Z","iopub.execute_input":"2025-04-06T14:45:36.616074Z","iopub.status.idle":"2025-04-06T14:45:45.529317Z","shell.execute_reply.started":"2025-04-06T14:45:36.616052Z","shell.execute_reply":"2025-04-06T14:45:45.528127Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-8ac61a9c4766>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_audio_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'filename'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_wav2vec2_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mX_train_feats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0my_train_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-5-831ca5cdfa8a>\u001b[0m in \u001b[0;36mextract_wav2vec2_embedding\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":7},{"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-06T14:38:49.150Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_ridge = Ridge(alpha=1.0)\nmodel_ridge.fit(X_train, y_train)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-06T14:38:49.150Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred = model_ridge.predict(X_val)\n\nrmse = np.sqrt(mean_squared_error(y_val, y_pred))\npearson_corr, _ = pearsonr(y_val, y_pred)\n\nprint(f\"Validation RMSE: {rmse:.4f}\")\nprint(f\"Pearson Correlation: {pearson_corr:.4f}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-06T14:38:49.151Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_test_feats = []\n\nfor idx, row in test_df.iterrows():\n    file_path = os.path.join(test_audio_path, row['filename'])\n    features = extract_wav2vec2_embedding(file_path)\n    X_test_feats.append(features)\n\nX_test = np.array(X_test_feats)\n\ntest_preds = model_ridge.predict(X_test)\n\nsubmission = test_df.copy()\nsubmission['label'] = test_preds\nsubmission[['filename','label']].to_csv(\"submission.csv\", index=False)\nsubmission.head()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-06T14:38:49.151Z"}},"outputs":[],"execution_count":null}]}